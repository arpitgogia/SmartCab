{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project 4: Train a Smartcab\n",
    "==========================\n",
    "\n",
    "Training a smartcab using Reinforcement Learning\n",
    "\n",
    "Task 1\n",
    "======\n",
    "### Implement a basic Learning Agent\n",
    "Actions are randomly choosen at each iteration using **```random.choice(available_actions)```**\n",
    "```\n",
    "if inputs['light'] == 'red':\n",
    "            \"\"\"\n",
    "                Red Light means :\n",
    "                    Left when no oncoming traffic\n",
    "                    No action\n",
    "            \"\"\"\n",
    "            if inputs['oncoming'] != 'left':\n",
    "                available_actions = [None, 'right']\n",
    "        else:\n",
    "            \"\"\"\n",
    "                Green Light means :\n",
    "                    Left only if no forward oncoming traffic\n",
    "                    Perform atleast some action\n",
    "            \"\"\"        \n",
    "            available_actions = ['right', 'left', 'forward']\n",
    "            if inputs['oncoming'] == 'forward':\n",
    "                available_actions.remove('left')\n",
    "        action = None\n",
    "        if available_actions != []:\n",
    "            action = random.choice(available_actions)\n",
    "```\n",
    "### ***Q1. Mention what you see in the agent’s behavior. Does it eventually make it to the target location?***\n",
    "The output as a result of the random movement algorithm (```/smartcab/test_random.txt```):\n",
    "<br>\n",
    ">**32 trips out of the 100, the cab reaches the destination.**<br>\n",
    "\n",
    "The agent is able to reach the destination more than 25% of the trips while still following all the traffic rules.\n",
    "It does eventually make it to the target location however since it wasn't behaving intelligently at any point of time, hence it has a very low success rate. This was when I set ```enforce_deadline = True```\n",
    "<br>\n",
    "When the deadline is not enforced, \n",
    ">**81 trips out of the 100, the cab reaches the destination.**<br>\n",
    "\n",
    "The behaviour was as expected because the randomised movements will take the cab to the destination however in majority of the cases not under the deadline. \n",
    "### ***Q2. Identify and Update State. Justify why you picked these set of states, and how they model the agent and its environment.***\n",
    "The state that I chose is represented by the input parameters, the next waypoint and the deadline. <br>\n",
    "I chose these parameters for my state because it represents the scenario of real life driving to a very good extent. In real life, while driving a cab, one considers oncoming traffic, lights etc. as well as in how much time one has to get there and the proposed route using waypoints. Another factor that I considered while choosing state variables was the availability of these in real life. The state variables that I chose are available through sensors in a self driving car.\n",
    "Task 2\n",
    "========\n",
    "### Implement Q Learning\n",
    "The Q Learning algorithm I followed is exactly as mentioned in the video Lectures. <br>\n",
    "The state is represented by the input parameters **```oncoming```** and **```light```**, and **```the next waypoint```**. These three represent the exact state of the cab at a point of time. The primary aim is to train the cab for performing legal moves. Legal moves result in the highest possible reward. Even in real life, following the traffic rules is slightly more important than reaching before the deadline. Therefore in this case I haven't included the deadline as a state variable.\n",
    "The Q Learning Algorithm is explained below : \n",
    "> 1. Construct the State Representation\n",
    "> 2. Obtain Action for that state\n",
    "    1. Random action is choosen with epsilon probability.\n",
    "    2. Otherwise choose action using the policy of greatest q value gives best action.\n",
    "> 3. Update Q Values according to reward by the action.\n",
    "\n",
    "### ***Q3. What changes do you notice in the agent’s behavior ?***\n",
    "After running ```agent.py``` I found out that the agent is now considerably more accurate than the random agent. Tests revealed a success rate of ***97/100***```(/smartcab/test_q_100.txt)```. In a seperate test with number of trials = 200,  the agent reached the target ***195/200***```(/smartcab/test_q_200.txt)``` times. The agent behaves intelligently at every decision point, based on the q values. Randomness has been minimised by setting a very low epsilon.\n",
    "\n",
    "### *** Q4. Report what changes you made to your basic implementation of Q-Learning to achieve the final version of the agent. How well does it perform?***\n",
    "The parameters of the initial implementation of the Q Learning Algorithm were totally random at **alpha = 0.5**, **gamma = 0.5**, **epsilon = 0.5**, and **initial Q Value = 20.0**. After a host of testing with various parameters, I arrived at the optimal parameters:\n",
    "* **alpha = 0.9**: High learning rate is desirable\n",
    "* **epsilon = 0.001**: Provides sufficient amount of randomness in the selection of an action. Our aim is to obtain maximum actions from the learning policy.\n",
    "* **gamma = 0.35**: Brute Force\n",
    "* **Initial Q Value = 19.75**: Brute Force<br>\n",
    "\n",
    "As mentioned above the algorithm performs better as compared to the random agent. **97 out of 100** is a remarkably good accuracy for a smart cab. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
