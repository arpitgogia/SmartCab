{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project 4: Train a Smartcab\n",
    "==========================\n",
    "\n",
    "Training a smartcab using Reinforcement Learning\n",
    "\n",
    "Task 1\n",
    "======\n",
    "### Implement a basic Learning Agent\n",
    "Actions are randomly choosen at each iteration using **```random.choice(available_actions)```**\n",
    "```\n",
    "if inputs['light'] == 'red':\n",
    "            \"\"\"\n",
    "                Red Light means :\n",
    "                    Left when no oncoming traffic\n",
    "                    No action\n",
    "            \"\"\"\n",
    "            if inputs['oncoming'] != 'left':\n",
    "                available_actions = [None, 'right']\n",
    "        else:\n",
    "            \"\"\"\n",
    "                Green Light means :\n",
    "                    Left only if no forward oncoming traffic\n",
    "                    Perform atleast some action\n",
    "            \"\"\"        \n",
    "            available_actions = ['right', 'left', 'forward']\n",
    "            if inputs['oncoming'] == 'forward':\n",
    "                available_actions.remove('left')\n",
    "        action = None\n",
    "        if available_actions != []:\n",
    "            action = random.choice(available_actions)\n",
    "```\n",
    "### ***Q1. Mention what you see in the agent’s behavior. Does it eventually make it to the target location?***\n",
    "The output as a result of the random movement algorithm (```/smartcab/test_random.txt```):\n",
    "<br>\n",
    ">**32 trips out of the 100, the cab reaches the destination.**<br>\n",
    "\n",
    "The agent is able to reach the destination more than 25% of the trips while still following all the traffic rules.\n",
    "It does eventually make it to the target location however since it wasn't behaving intelligently at any point of time, hence it has a very low success rate. This was when I set ```enforce_deadline = True```\n",
    "<br>\n",
    "When the deadline is not enforced, \n",
    ">**81 trips out of the 100, the cab reaches the destination.**<br>\n",
    "\n",
    "The behaviour was as expected because the randomised movements will take the cab to the destination however in majority of the cases not under the deadline. \n",
    "### ***Q2. Identify and Update State. Justify why you picked these set of states, and how they model the agent and its environment.***\n",
    "I chose the parameters **```oncoming```**, **```left```**, **```light```**, and **```the next waypoint```** to represent the state of the agent at an instance. I chose these parameters because:\n",
    "* **```oncoming```** and **```left```**: The agent needs to know about any traffic which is oncoming or coming in from the left as it has to yield to cars on these roads according to the traffic rules.\n",
    "* **```light```**: Very important paramter as the first motive of our agent is to follow the traffic rules.\n",
    "* **```next_waypoint```**: For knowledge of the route the agent has to take.\n",
    "\n",
    "The agent can be modeled using the above state variables, efficiently.\n",
    "I believe that the deadline shouldn't be big enough factor to make it a part of the state. Our primary concern with the agent is to follow the traffic rules. Even in real life traffic rules are a higher priority than reaching somewhere within a time. I also believe that the agent learns to follow the optimised path after sufficient learning. Another fact is that taking the deadline as part of the state increase the size of the state space. This makes learning more difficult.\n",
    "Task 2\n",
    "========\n",
    "### Implement Q Learning\n",
    "The Q Learning algorithm I followed is exactly as mentioned in the video Lectures. <br>\n",
    "The state is represented by the input parameters **```oncoming```** and **```light```**, and **```the next waypoint```**. These three represent the exact state of the cab at a point of time. The primary aim is to train the cab for performing legal moves. Legal moves result in the highest possible reward. Even in real life, following the traffic rules is slightly more important than reaching before the deadline. Therefore in this case I haven't included the deadline as a state variable.\n",
    "The Q Learning Algorithm is explained below : \n",
    "> 1. Construct the State Representation\n",
    "> 2. Obtain Action for that state\n",
    "    1. Random action is choosen with epsilon probability.\n",
    "    2. Otherwise choose action using the policy of greatest q value gives best action.\n",
    "> 3. Update Q Values according to reward by the action.\n",
    "\n",
    "### ***Q3. What changes do you notice in the agent’s behavior ?***\n",
    "After running ```agent.py``` I found out that the agent is now considerably more accurate than the random agent. Tests revealed a success rate of ***97/100***```(/smartcab/test_q_100.txt)```. In a seperate test with number of trials = 200,  the agent reached the target ***195/200***```(/smartcab/test_q_200.txt)``` times. The agent behaves intelligently at every decision point, based on the q values. Randomness has been minimised by setting a very low epsilon. <br>\n",
    "To get a more clear picture I have plotted the number of negative rewards in each trial of the Random Agent along with the Q Learning Enabled Agent.\n",
    "\n",
    "### *** Q4. Report what changes you made to your basic implementation of Q-Learning to achieve the final version of the agent. How well does it perform?***\n",
    "The parameters of the initial implementation of the Q Learning Algorithm were totally random at **alpha = 0.5**, **gamma = 0.5**, **epsilon = 0.5**, and **initial Q Value = 20.0**. \n",
    "Following table shows the various parameters I tested the Q LEarning agent with: \n",
    "\n",
    "|**S.No.**|**Alpha**|**Gamma**|**Epsilon**|**Q_Initial**|**Successful Trips (out of 100)**|\n",
    "|:-------:|:-------:|:-------:|:---------:|:-----------:|:-------------------------------:|\n",
    "|1|0.5|0.5|0.5|20.0|57|\n",
    "|2|0.6|0.4|0.25|20.0|82|\n",
    "|3|0.8|0.1|0.125|19.75|93|\n",
    "|2|0.9|0.35|0.001|19.75|97|\n",
    "\n",
    "After a host of testing with various parameters, I arrived at the optimal parameters:\n",
    "* **alpha = 0.9**: High learning rate is desirable\n",
    "* **epsilon = 0.001**: Provides sufficient amount of randomness in the selection of an action. Our aim is to obtain maximum actions from the learning policy.\n",
    "* **gamma = 0.35**: Brute Force\n",
    "* **Initial Q Value = 19.75**: Brute Force<br>\n",
    "\n",
    "As mentioned above the algorithm performs better as compared to the random agent. **97 out of 100** is a remarkably good accuracy for a smart cab. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
