{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project 4: Train a Smartcab\n",
    "==========================\n",
    "\n",
    "Training a smartcab using Reinforcement Learning\n",
    "\n",
    "Task 1\n",
    "======\n",
    "### Implement a basic Learning Agent\n",
    "Actions are randomly choosen at each iteration using **```random.choice(available_actions)```**\n",
    "```\n",
    "if inputs['light'] == 'red':\n",
    "            \"\"\"\n",
    "                Red Light means :\n",
    "                    Left when no oncoming traffic\n",
    "                    No action\n",
    "            \"\"\"\n",
    "            if inputs['oncoming'] != 'left':\n",
    "                available_actions = [None, 'right']\n",
    "        else:\n",
    "            \"\"\"\n",
    "                Green Light means :\n",
    "                    Left only if no forward oncoming traffic\n",
    "                    Perform atleast some action\n",
    "            \"\"\"        \n",
    "            available_actions = ['right', 'left', 'forward']\n",
    "            if inputs['oncoming'] == 'forward':\n",
    "                available_actions.remove('left')\n",
    "        action = None\n",
    "        if available_actions != []:\n",
    "            action = random.choice(available_actions)\n",
    "```\n",
    "### ***Q1. Mention what you see in the agent’s behavior. Does it eventually make it to the target location?***\n",
    "The output as a result of the random movement algorithm (```/smartcab/test_random.txt```):\n",
    "<br>\n",
    ">**32 trips out of the 100, the cab reaches the destination.**<br>\n",
    "\n",
    "The agent is able to reach the destination more than 25% of the trips while still following all the traffic rules.\n",
    "### ***Q2. Identify and Update State. Justify why you picked these set of states, and how they model the agent and its environment.***\n",
    "The state that I chose is represented by the input parameters, the next waypoint and the deadline. <br>\n",
    "I chose these parameters for my state because it represents the scenario of real life driving to a very good extent. In real life, while driving a cab, one considers oncoming traffic, lights etc. as well as in how much time one has to get there and the proposed route using waypoints. Another factor that I considered while choosing state variables was the availability of these in real life. The state variables that I chose are available through sensors in a self driving car.\n",
    "Task 2\n",
    "========\n",
    "### Implement Q Learning\n",
    "The Q Learning algorithm I followed is similar to what has been mentioned in the video Lectures. <br>\n",
    "The state is represented by the **dictionary of inputs, next waypoint and the deadline**. These three represent the exact state of the cab at a point of time. The primary aim is to train the cab for performing legal moves. Legal moves result in the highest possible reward. Even in real life, following the traffic rules is slightly more important than reaching before the deadline. However the deadline is still important for the cab to recognise it as part of the cab's enovironment.\n",
    "The Q Learning Algorithm is explained below : \n",
    "> 1. Construct the State Representation\n",
    "> 2. Obtain Action for that state\n",
    "    1. Epsilon probability of a random action is choosen.\n",
    "    2. Otherwise choose action using the policy of greatest q value gives best action.\n",
    "> 3. Update Q Values according to reward by the action.\n",
    "\n",
    "### ***Q3. What changes do you notice in the agent’s behavior ?***\n",
    "After running ```agent.py``` I found out that the agent is now considerably more accurate than the random agent. Tests revealed a success rate of ***41/100***. \n",
    "\n",
    "### *** Q4. Report what changes you made to your basic implementation of Q-Learning to achieve the final version of the agent. How well does it perform?***\n",
    "The parameters of the initial implementation of the Q Learning Algorithm were totally random at **alpha = 0.5**, **gamma = 0.5**, **epsilon = 0.5**, and **initial Q Value = 20.0**. After a host of testing with various parameters, I arrived at the optimal parameters:\n",
    "* **alpha = 0.9**: High learning rate is desirable\n",
    "* **epsilon = 0.001**: Provides sufficient amount of randomness in the selection\n",
    "* **gamma = 0.35**: Brute Force\n",
    "* **Initial Q Value = 19.75**: Brute Force"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
